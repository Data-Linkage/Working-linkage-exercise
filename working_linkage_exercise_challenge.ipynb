{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "included-beatles",
   "metadata": {},
   "source": [
    "# Data Linkage course: coding exercise 1\n",
    "\n",
    "We have two datasets which we want to link: **working_data_a** and **working_data_b**\n",
    "\n",
    " - **working_data_a** contains the variables: `id_a`, `firstname`, `middlename`, `surname`, `sex`, `dob`, `postcode` and a record ID that is contained in the variable: `ident_b`. In addition there is a variable, `ident_a`,  that contains the record ID from the small file that it is matched to, i.e. we know the true match status.\n",
    "\n",
    " - **working_data_b** contains the variables: `id_b`, `firstname`, `middlename`, `surname`, `sex`, `dob`, `postcode` and a record ID that is contained in the variable: `ident_a`. In addition there is a variable, `ident_b`,  that contains the record ID from the small file that it is matched to, i.e. we know the true match status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation, numpy for filtering, os to read the working directory and re for regular expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Modify the settings so any variable or statement on its own line is displayed\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Widen output display\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path():\n",
    "    # This is the filepath where the datasets and the matchkey file can be found\n",
    "    file_path = os.getcwd()\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    # Read in datasets to link\n",
    "    dfA = pd.read_csv(filepath + '/working_data_a.csv')\n",
    "    dfB = pd.read_csv(filepath + '/working_data_b.csv')\n",
    "    # Make sure column types correct\n",
    "    dfA = dfA.astype({\"id_a\":str, \"firstname_a\":str, \"middlename_a\":str, \"surname_a\":str,\n",
    "                      \"sex_a\":str, \"dob_a\":str, \"postcode_a\": str})\n",
    "    \n",
    "    dfB = dfB.astype({\"id_b\":str, \"firstname_b\":str, \"middlename_b\":str, \"surname_b\":str,\n",
    "                      \"sex_b\":str, \"dob_b\":str, \"postcode_b\": str})\n",
    "    # Drop the ident_a and ident_b columns as we don't need them\n",
    "    return (dfA.drop(['ident_a','ident_b'], axis=1), dfB.drop(['ident_a','ident_b'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, letter):\n",
    "    # Convert standardise the name variables\n",
    "    df = standardise_names(df, letter)\n",
    "    # Standardise the postcodes\n",
    "    df = standardise_postcode(df, 'postcode_' + letter)\n",
    "    # Standardise the sex\n",
    "    df = standardise_sex(df, 'sex_' + letter)\n",
    "    # Standardise the date of birth\n",
    "    df = standardise_dob(df, 'dob_' + letter)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_names(df, letter):\n",
    "    for name in add_subscript(letter, [\"firstname\", \"middlename\", \"surname\"]):\n",
    "        '''Can you define this function logic to standardise name variables by:\n",
    "        \n",
    "        - Setting all characters to uppercase\n",
    "        - Ensuring names have no leading or trailing whitespaces, and only single spaces internally\n",
    "        - Converting all hyphens to spaces\n",
    "        - Replacing empty or \"NAN\" names with \"None\"\n",
    "        \n",
    "        Pandas series.str functions could help with the text editing here.\n",
    "        Also, numpy.where could return name elements based on certain conditions.\n",
    "        '''\n",
    "        # df[name] = Your_answer_here\n",
    "        \n",
    "        # Convert the column type to str\n",
    "        df.astype({name:str}, copy=False)\n",
    "    # Split each name into two variables on the delimiter ' '. Later name variables\n",
    "    # will be 'None' if there are not two names in that column. Titles will be \n",
    "    # stripped off into a title column\n",
    "    return split_names(df, letter)\n",
    "\n",
    "def split_names(df, letter):  \n",
    "    # Split firstname. As it may contain a title and two names, we need to temporarily\n",
    "    # create three firstname variables (the last of which will be dropped later)\n",
    "    f1, f2, f3 = add_subscript(letter, ['first1', 'first2','first3'])\n",
    "    df[[f1, f2, f3]] = df['firstname_'+letter].str.split(' ', n=2, expand=True)\n",
    "    # Remove any titles by putting them in a separate title column\n",
    "    titles = ['MR', 'MRS', 'MISS', 'MS', 'DR']\n",
    "    df['title_'+letter] = np.where(df[f1].isin(titles), df[f1], None)\n",
    "    # If firstname1 is a title, swap the order of the forename variables so it\n",
    "    # becomes forename3 (the column we will drop)\n",
    "    df[f1], df[f2] = np.where(df[f1].isin(titles), [df[f2],df[f1]], [df[f1],df[f2]])\n",
    "    df[f2], df[f3] = np.where(df[f2].isin(titles), [df[f3],df[f2]], [df[f2],df[f3]])\n",
    "    # Split middlename\n",
    "    df[add_subscript(letter, ['middle1', 'middle2'])] = df['middlename_'+letter].str.split(' ', n=1, expand=True)\n",
    "    # Split surname\n",
    "    df['sur1_'+letter], df['sur2_'+letter] = df['surname_'+letter], None\n",
    "    #df[add_subscript(letter, ['sur1', 'sur2'])] = df['surname_'+letter].str.split(' ', n=1, expand=True)\n",
    "    # Firstname3 no longer needed as it will only contain titles\n",
    "    return df.drop(f3, axis=1)\n",
    "\n",
    "\n",
    "def standardise_postcode(df, pc):\n",
    "    '''\n",
    "    Can you remove all white space from the postcodes and make them uppercase below?\n",
    "    Pandas: Series.str functions could resolve this\n",
    "    '''\n",
    "    # df[pc] = Your_answer_here\n",
    "    \n",
    "    ''' Replace missing postcodes (NAN or '' or null) with 'None'\n",
    "    Numpy.where functions may help\n",
    "    '''\n",
    "    # df[pc] = Your_answer_here\n",
    "    \n",
    "    # Convert the type to string\n",
    "    return df.astype({pc:str})\n",
    "\n",
    "\n",
    "def standardise_sex(df, sex):\n",
    "    # Convert the sex to uppercase\n",
    "    df[sex] = df[sex].str.upper()\n",
    "    # Change 'M' and 'MALE' to 1 and 'F' and 'FEMALE' to 2\n",
    "    df.loc[(df[sex] == 'M') | (df[sex] == 'MALE'), sex] = 1\n",
    "    df.loc[(df[sex] == 'F') | (df[sex] == 'FEMALE'), sex] = 2\n",
    "    # Convert the type to a float\n",
    "    return df.astype({sex:float})\n",
    "\n",
    "\n",
    "def standardise_dob(df, dob):\n",
    "    df[dob] = pd.to_datetime(df[dob], dayfirst=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_derived_variables(df, letter):\n",
    "    # Make new columns containing only the first three letters and initials of each\n",
    "    # part of each name. Also create a column for initials\n",
    "    df = short_names(df, letter)\n",
    "    # Create day, month and year variables for each dataset\n",
    "    dob = 'dob_' + letter\n",
    "    df['daybirth_'+letter] = df[dob].dt.day\n",
    "    df['monthbirth_'+letter] = df[dob].dt.month\n",
    "    df['yearbirth_'+letter] = df[dob].dt.year\n",
    "    # Separate out the different parts of the postcode\n",
    "    return split_postcode(df, letter)\n",
    "   \n",
    "    \n",
    "def short_names(df, letter):\n",
    "    # Create columns containing the first three letters and initial of each name\n",
    "    f1, f2, m1, m2, s1, s2 = add_subscript(letter, ['first1', 'first2', 'middle1', 'middle2', 'sur1', 'sur2'])\n",
    "    for name in [f1, f2, m1, m2, s1, s2]:\n",
    "        df['short'+name] = df[name].str[:3]\n",
    "        df['init'+name] = df[name].str[0]\n",
    "    # Combine all of the initial columns to give a person's initials\n",
    "    df['initials_'+letter] = df[['init'+f1,'init'+f2,'init'+m1,'init'+m2,'init'+s1,'init'+s2]].apply(\n",
    "            lambda row: row.str.cat(sep=''), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_postcode(df, letter):\n",
    "    # Split out the postcode area and district\n",
    "    pc, area, district = add_subscript(letter, ['postcode', 'pcarea', 'pcdistrict'])\n",
    "    df[area] = df[pc].str.extract('\\A([A-Z]{1,2})', expand=True)\n",
    "    df[district] = df[pc].str.extract('\\A([A-Z]{1,2}[0-9]{1,2})[0-9]', expand=True)\n",
    "    df[district] = np.where((df[district].isnull()) & (df[pc].str.match('[A-Z]{1,2}[0-9]{1,2}')),\n",
    "                              df[pc], df[district])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subscript(letter, args):\n",
    "    # Add the letter as a subscript to each variable\n",
    "    args_subscript = []\n",
    "    for arg in args:\n",
    "        args_subscript.append(str(arg) + '_' + letter)\n",
    "    return args_subscript\n",
    "\n",
    "\n",
    "def remove_subscript(args):\n",
    "    # Remove the subscript from each variable\n",
    "    args_no_subscript = []\n",
    "    for arg in args:\n",
    "        args_no_subscript.append(str(arg).split('_')[0])\n",
    "    return args_no_subscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_matching(dfA, dfB):\n",
    "    # Exact matching is just rule-based matching on all of the columns\n",
    "    return rule_based_matching(dfA, dfB, remove_subscript(dfA.columns.values), False)\n",
    "\n",
    "\n",
    "def create_matchkeys(filepath):\n",
    "    # Read in the matchkeys file\n",
    "    file = open(filepath + '/working_matchkeys.txt')\n",
    "    lines = file.read().splitlines()\n",
    "    file.close()\n",
    "    matchkeys = []\n",
    "    keep_multiple = False\n",
    "    for line in lines:\n",
    "        if re.match('Include multiple matches', line):\n",
    "            # Set the value of keep_multiple to True or False as appropriate. If neither\n",
    "            # True or False was inputted, raise a value error.\n",
    "            value = re.search('Include multiple matches *=(.*)', line).group(1).replace(' ','')\n",
    "            if value.upper() == 'TRUE':\n",
    "                keep_multiple = True\n",
    "            elif value.replace(' ','').upper() == 'FALSE':\n",
    "                keep_multiple = False\n",
    "            else:\n",
    "                raise ValueError('\"Include multiple matches\" in the file matchkeys.txt '+\\\n",
    "                                 'must be either \"true\" or \"false\" but received \"' + value + '\"')\n",
    "        elif line == '':\n",
    "            # Do nothing as it's a blank line\n",
    "            continue\n",
    "        else:\n",
    "            # Add the matchkey to the list of matchkeys\n",
    "            matchkeys.append(line.replace(' ','').split(','))\n",
    "    return keep_multiple, matchkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_matching(dfA, dfB, matchkey, keep_multiple):\n",
    "    # Create the variable names for dfA and dfB\n",
    "    left = add_subscript('a', matchkey)\n",
    "    right = add_subscript('b', matchkey)\n",
    "    # Join on the columns given by args\n",
    "    linked = dfA.merge(dfB, left_on = left, right_on = right, how = 'inner')\n",
    "    multiple_matches = None\n",
    "    if keep_multiple == False:\n",
    "        # Remove the multiple matches\n",
    "        linked['multi_match'] = linked['id_a'].map(linked['id_a'].value_counts()>1)\\\n",
    "                                | linked['id_b'].map(linked['id_b'].value_counts()>1)\n",
    "        multiple_matches = len(linked[linked['multi_match']])\n",
    "        linked = linked[linked['multi_match'] == False].drop('multi_match', axis=1)\n",
    "    # Add a column 'Match_Status' that has value 1 if the match is correct and 0 otherwise\n",
    "    linked['Match_Status'] = np.where(linked['id_a'] == linked['id_b'],1,0)\n",
    "    true_positives = len(linked[linked['Match_Status']==1])\n",
    "    false_positives = len(linked[linked['Match_Status']==0])\n",
    "    # Find the residuals\n",
    "    residuals = dfA.merge(dfB, left_on = left, right_on = right, how = 'outer')\n",
    "    # Calculate residuals from dfA. We want to include all records from dfA that\n",
    "    # are not in linked but we don't want to include any records from dfB (as\n",
    "    # these would just be a row of null values)\n",
    "    residualsA = residuals[residuals['id_a'].notnull() &\n",
    "                           ~residuals['id_a'].isin(linked['id_a'])][dfA.columns.values]\\\n",
    "                           .drop_duplicates()\n",
    "    # Calculate residuals from dfB. We want to include all records from dfB that\n",
    "    # are not in linked but we don't want to include any records from dfA (as\n",
    "    # these would just be a row of null values)\n",
    "    residualsB = residuals[residuals['id_b'].notnull() & \n",
    "                           ~residuals['id_b'].isin(linked['id_b'])][dfB.columns.values]\\\n",
    "                           .drop_duplicates()\n",
    "    unmatched = len(residualsB)\n",
    "    # Print information on the number of true positives, false positives and \n",
    "    # unmatched records for this matchkey\n",
    "    print_match_rate(False, left == dfA.columns.values.tolist(), matchkey,\n",
    "                     multiple_matches, true_positives, false_positives, unmatched, None)\n",
    "    # Update dfA and dfB to remove the matched records\n",
    "    dfA = residualsA\n",
    "    dfB = residualsB\n",
    "    return (dfA, dfB, (true_positives, false_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_match_rate(final, exact, matchkey, multiple_matches, true_positives,\n",
    "                     false_positives, unmatched, false_negatives):\n",
    "    # Print details of the type of matching and the number of true positives, false\n",
    "    # positives etc. in a readable form\n",
    "    if final:\n",
    "        print('Overall match rate')\n",
    "    elif exact:\n",
    "        print('Exact matching')\n",
    "    else:\n",
    "        print('Matchkey = ' + ', '.join(matchkey))\n",
    "        if multiple_matches != None:\n",
    "            print('Multiple matches = ' + str(multiple_matches))\n",
    "    print('True positives = ' + str(true_positives))\n",
    "    print('False positives = ' + str(false_positives))\n",
    "    if unmatched != None:\n",
    "        print('Unmatched = ' + str(unmatched))\n",
    "    if false_negatives != None:\n",
    "        print('False negatives = ' + str(false_negatives))\n",
    "    print('')\n",
    "\n",
    "\n",
    "def update_match_rate(true_positives, false_positives, link_status):\n",
    "    # Update the number of true positives and false positives found\n",
    "    true_positives += link_status[0]\n",
    "    false_positives += link_status[1]\n",
    "    return(true_positives, false_positives)\n",
    "\n",
    "\n",
    "def get_precision(true_positives, false_positives):\n",
    "    # Calculate the precision as a percentage to 2 decimal places\n",
    "    return round(true_positives/(true_positives+false_positives)*100, 2)\n",
    "\n",
    "\n",
    "def get_recall(true_positives, false_negatives):\n",
    "    # Calculate the recall as a percentage to 2 decimal places\n",
    "    return round(true_positives/(true_positives+false_negatives)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepath from the command line arguments\n",
    "filepath = get_file_path()\n",
    "\n",
    "# Read in the data, clean it and create any derived variables so it is ready for matching\n",
    "dfA, dfB = read_data(filepath)\n",
    "dfA = clean_data(dfA, \"a\")\n",
    "dfB = clean_data(dfB, \"b\")\n",
    "dfA = create_derived_variables(dfA, \"a\")\n",
    "dfB = create_derived_variables(dfB, \"b\")\n",
    "   \n",
    "# Set the number of false negatives to be the length of dataset B and the number \n",
    "# of true positives and false positives to be zero as we haven't made any matches\n",
    "false_neg = len(dfB)\n",
    "true_pos, false_pos = 0, 0\n",
    "    \n",
    "# Run the exact matching and update the number of true positives and false positives\n",
    "dfA, dfB, link_status = exact_matching(dfA, dfB)\n",
    "true_pos, false_pos = update_match_rate(true_pos, false_pos, link_status)\n",
    "    \n",
    "# Read in the list of matchkeys and convert each to a list of variables on which to match\n",
    "keep_multiple, matchkeys = create_matchkeys(filepath)\n",
    "    \n",
    "# For each matchkey, run the rule-based matching and then update the number of \n",
    "# true positives and false positives\n",
    "for matchkey in matchkeys:\n",
    "    dfA, dfB, link_status = rule_based_matching(dfA, dfB, matchkey, keep_multiple)\n",
    "    true_pos, false_pos = update_match_rate(true_pos, false_pos, link_status)\n",
    "    \n",
    "# Calculate the number of false negatives\n",
    "false_neg -= true_pos\n",
    "    \n",
    "# Print overall summary information and calculate the precision and recall\n",
    "print_match_rate(True, False, None, None, true_pos, false_pos, None, false_neg)\n",
    "print('Precision = ' + str(get_precision(true_pos, false_pos)) + '%')\n",
    "print('Recall = ' + str(get_recall(true_pos, false_neg)) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
